{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa362d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAPI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3798fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: list_files\n",
      "Tool Arguments: {}\n",
      "Result: ['function-apis.ipynb', 'colab-sample.ipynb', 'python-calls.ipynb', '.env', 'module-1-example.ipynb', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "from litellm import completion\n",
    "\n",
    "def list_files() -> List[str]:\n",
    "    \"\"\"List files in the current directory.\"\"\"\n",
    "    return os.listdir(\".\")\n",
    "\n",
    "def read_file(file_name: str) -> str:\n",
    "    \"\"\"Read a file's contents.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: {file_name} not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "tool_functions = {\n",
    "    \"list_files\": list_files,\n",
    "    \"read_file\": read_file\n",
    "}\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"list_files\",\n",
    "            \"description\": \"Returns a list of files in the directory.\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"read_file\",\n",
    "            \"description\": \"Reads the content of a specified file in the directory.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"file_name\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"file_name\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Our rules are simplified since we don't have to worry about getting a specific output format\n",
    "agent_rules = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "You are an AI agent that can perform tasks by using available tools. \n",
    "\n",
    "If a user asks about files, documents, or content, first list the files before reading them.\n",
    "\"\"\"\n",
    "}]\n",
    "\n",
    "user_task = input(\"What would you like me to do? \")\n",
    "\n",
    "memory = [{\"role\": \"user\", \"content\": user_task}]\n",
    "\n",
    "messages = agent_rules + memory\n",
    "\n",
    "response = completion(\n",
    "    model=\"openai/gpt-4o\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# Extract the tool call from the response, note we don't have to parse now!\n",
    "tool = response.choices[0].message.tool_calls[0]\n",
    "tool_name = tool.function.name\n",
    "tool_args = json.loads(tool.function.arguments)\n",
    "result = tool_functions[tool_name](**tool_args)\n",
    "\n",
    "print(f\"Tool Name: {tool_name}\")\n",
    "print(f\"Tool Arguments: {tool_args}\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8067b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Choices is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m tool_args \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(tool\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39marguments)\n\u001b[1;32m     77\u001b[0m result \u001b[38;5;241m=\u001b[39m tool_functions[tool_name](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_args)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     80\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m api_ke\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Choices is not JSON serializable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5a0f41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: list_files with args {}\n",
      "Result: {'result': ['function-apis.ipynb', 'colab-sample.ipynb', 'python-calls.ipynb', '.env', 'module-1-example.ipynb', '.ipynb_checkpoints']}\n",
      "Executing: read_file with args {'file_name': 'function-apis.ipynb'}\n",
      "Result: {'result': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"id\": \"0aa362d8\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import os\\\\n\",\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv()\\\\n\",\\n    \"api_key = os.getenv(\\'OPENAPI_API_KEY\\')\\\\n\",\\n    \"os.environ[\\'OPENAI_API_KEY\\'] = api_key\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 4,\\n   \"id\": \"4e3798fc\",\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Tool Name: list_files\\\\n\",\\n      \"Tool Arguments: {}\\\\n\",\\n      \"Result: [\\'function-apis.ipynb\\', \\'colab-sample.ipynb\\', \\'python-calls.ipynb\\', \\'.env\\', \\'module-1-example.ipynb\\', \\'.ipynb_checkpoints\\']\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"import json\\\\n\",\\n    \"import os\\\\n\",\\n    \"from typing import List\\\\n\",\\n    \"\\\\n\",\\n    \"from litellm import completion\\\\n\",\\n    \"\\\\n\",\\n    \"def list_files() -> List[str]:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"List files in the current directory.\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    return os.listdir(\\\\\".\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"def read_file(file_name: str) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Read a file\\'s contents.\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    try:\\\\n\",\\n    \"        with open(file_name, \\\\\"r\\\\\") as file:\\\\n\",\\n    \"            return file.read()\\\\n\",\\n    \"    except FileNotFoundError:\\\\n\",\\n    \"        return f\\\\\"Error: {file_name} not found.\\\\\"\\\\n\",\\n    \"    except Exception as e:\\\\n\",\\n    \"        return f\\\\\"Error: {str(e)}\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"tool_functions = {\\\\n\",\\n    \"    \\\\\"list_files\\\\\": list_files,\\\\n\",\\n    \"    \\\\\"read_file\\\\\": read_file\\\\n\",\\n    \"}\\\\n\",\\n    \"\\\\n\",\\n    \"tools = [\\\\n\",\\n    \"    {\\\\n\",\\n    \"        \\\\\"type\\\\\": \\\\\"function\\\\\",\\\\n\",\\n    \"        \\\\\"function\\\\\": {\\\\n\",\\n    \"            \\\\\"name\\\\\": \\\\\"list_files\\\\\",\\\\n\",\\n    \"            \\\\\"description\\\\\": \\\\\"Returns a list of files in the directory.\\\\\",\\\\n\",\\n    \"            \\\\\"parameters\\\\\": {\\\\\"type\\\\\": \\\\\"object\\\\\", \\\\\"properties\\\\\": {}, \\\\\"required\\\\\": []}\\\\n\",\\n    \"        }\\\\n\",\\n    \"    },\\\\n\",\\n    \"    {\\\\n\",\\n    \"        \\\\\"type\\\\\": \\\\\"function\\\\\",\\\\n\",\\n    \"        \\\\\"function\\\\\": {\\\\n\",\\n    \"            \\\\\"name\\\\\": \\\\\"read_file\\\\\",\\\\n\",\\n    \"            \\\\\"description\\\\\": \\\\\"Reads the content of a specified file in the directory.\\\\\",\\\\n\",\\n    \"            \\\\\"parameters\\\\\": {\\\\n\",\\n    \"                \\\\\"type\\\\\": \\\\\"object\\\\\",\\\\n\",\\n    \"                \\\\\"properties\\\\\": {\\\\\"file_name\\\\\": {\\\\\"type\\\\\": \\\\\"string\\\\\"}},\\\\n\",\\n    \"                \\\\\"required\\\\\": [\\\\\"file_name\\\\\"]\\\\n\",\\n    \"            }\\\\n\",\\n    \"        }\\\\n\",\\n    \"    }\\\\n\",\\n    \"]\\\\n\",\\n    \"\\\\n\",\\n    \"# Our rules are simplified since we don\\'t have to worry about getting a specific output format\\\\n\",\\n    \"agent_rules = [{\\\\n\",\\n    \"    \\\\\"role\\\\\": \\\\\"system\\\\\",\\\\n\",\\n    \"    \\\\\"content\\\\\": \\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"You are an AI agent that can perform tasks by using available tools. \\\\n\",\\n    \"\\\\n\",\\n    \"If a user asks about files, documents, or content, first list the files before reading them.\\\\n\",\\n    \"\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"}]\\\\n\",\\n    \"\\\\n\",\\n    \"user_task = input(\\\\\"What would you like me to do? \\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"memory = [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": user_task}]\\\\n\",\\n    \"\\\\n\",\\n    \"messages = agent_rules + memory\\\\n\",\\n    \"\\\\n\",\\n    \"response = completion(\\\\n\",\\n    \"    model=\\\\\"openai/gpt-4o\\\\\",\\\\n\",\\n    \"    messages=messages,\\\\n\",\\n    \"    tools=tools,\\\\n\",\\n    \"    max_tokens=1024\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"# Extract the tool call from the response, note we don\\'t have to parse now!\\\\n\",\\n    \"tool = response.choices[0].message.tool_calls[0]\\\\n\",\\n    \"tool_name = tool.function.name\\\\n\",\\n    \"tool_args = json.loads(tool.function.arguments)\\\\n\",\\n    \"result = tool_functions[tool_name](**tool_args)\\\\n\",\\n    \"\\\\n\",\\n    \"print(f\\\\\"Tool Name: {tool_name}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Tool Arguments: {tool_args}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Result: {result}\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"id\": \"5a8067b6\",\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"ename\": \"TypeError\",\\n     \"evalue\": \"Object of type ModelResponse is not JSON serializable\",\\n     \"output_type\": \"error\",\\n     \"traceback\": [\\n      \"\\\\u001b[0;31m---------------------------------------------------------------------------\\\\u001b[0m\",\\n      \"\\\\u001b[0;31mTypeError\\\\u001b[0m                                 Traceback (most recent call last)\",\\n      \"Cell \\\\u001b[0;32mIn[8], line 79\\\\u001b[0m\\\\n\\\\u001b[1;32m     76\\\\u001b[0m tool_args \\\\u001b[38;5;241m=\\\\u001b[39m json\\\\u001b[38;5;241m.\\\\u001b[39mloads(tool\\\\u001b[38;5;241m.\\\\u001b[39mfunction\\\\u001b[38;5;241m.\\\\u001b[39marguments)\\\\n\\\\u001b[1;32m     77\\\\u001b[0m result \\\\u001b[38;5;241m=\\\\u001b[39m tool_functions[tool_name](\\\\u001b[38;5;241m*\\\\u001b[39m\\\\u001b[38;5;241m*\\\\u001b[39mtool_args)\\\\n\\\\u001b[0;32m---> 79\\\\u001b[0m \\\\u001b[38;5;28mprint\\\\u001b[39m(\\\\u001b[43mjson\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mdumps\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\u001b[43mresponse\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mindent\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[38;5;241;43m2\\\\u001b[39;49m\\\\u001b[43m)\\\\u001b[49m)\\\\n\\\\u001b[1;32m     80\\\\u001b[0m os\\\\u001b[38;5;241m.\\\\u001b[39menviron[\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mOPENAI_API_KEY\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m] \\\\u001b[38;5;241m=\\\\u001b[39m api_ke\\\\n\\\\u001b[1;32m     81\\\\u001b[0m \\\\u001b[38;5;28mprint\\\\u001b[39m(\\\\u001b[38;5;124mf\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mTool Name: \\\\u001b[39m\\\\u001b[38;5;132;01m{\\\\u001b[39;00mtool_name\\\\u001b[38;5;132;01m}\\\\u001b[39;00m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m)\\\\n\",\\n      \"File \\\\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py:234\\\\u001b[0m, in \\\\u001b[0;36mdumps\\\\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\\\\u001b[0m\\\\n\\\\u001b[1;32m    232\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;28mcls\\\\u001b[39m \\\\u001b[38;5;129;01mis\\\\u001b[39;00m \\\\u001b[38;5;28;01mNone\\\\u001b[39;00m:\\\\n\\\\u001b[1;32m    233\\\\u001b[0m     \\\\u001b[38;5;28mcls\\\\u001b[39m \\\\u001b[38;5;241m=\\\\u001b[39m JSONEncoder\\\\n\\\\u001b[0;32m--> 234\\\\u001b[0m \\\\u001b[38;5;28;01mreturn\\\\u001b[39;00m \\\\u001b[38;5;28;43mcls\\\\u001b[39;49m\\\\u001b[43m(\\\\u001b[49m\\\\n\\\\u001b[1;32m    235\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mskipkeys\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mskipkeys\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mensure_ascii\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mensure_ascii\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[1;32m    236\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mcheck_circular\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mcheck_circular\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mallow_nan\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mallow_nan\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mindent\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mindent\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[1;32m    237\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[43mseparators\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mseparators\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43mdefault\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43mdefault\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\u001b[43m \\\\u001b[49m\\\\u001b[43msort_keys\\\\u001b[49m\\\\u001b[38;5;241;43m=\\\\u001b[39;49m\\\\u001b[43msort_keys\\\\u001b[49m\\\\u001b[43m,\\\\u001b[49m\\\\n\\\\u001b[1;32m    238\\\\u001b[0m \\\\u001b[43m    \\\\u001b[49m\\\\u001b[38;5;241;43m*\\\\u001b[39;49m\\\\u001b[38;5;241;43m*\\\\u001b[39;49m\\\\u001b[43mkw\\\\u001b[49m\\\\u001b[43m)\\\\u001b[49m\\\\u001b[38;5;241;43m.\\\\u001b[39;49m\\\\u001b[43mencode\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\u001b[43mobj\\\\u001b[49m\\\\u001b[43m)\\\\u001b[49m\\\\n\",\\n      \"File \\\\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/encoder.py:201\\\\u001b[0m, in \\\\u001b[0;36mJSONEncoder.encode\\\\u001b[0;34m(self, o)\\\\u001b[0m\\\\n\\\\u001b[1;32m    199\\\\u001b[0m chunks \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[38;5;28mself\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39miterencode(o, _one_shot\\\\u001b[38;5;241m=\\\\u001b[39m\\\\u001b[38;5;28;01mTrue\\\\u001b[39;00m)\\\\n\\\\u001b[1;32m    200\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m \\\\u001b[38;5;129;01mnot\\\\u001b[39;00m \\\\u001b[38;5;28misinstance\\\\u001b[39m(chunks, (\\\\u001b[38;5;28mlist\\\\u001b[39m, \\\\u001b[38;5;28mtuple\\\\u001b[39m)):\\\\n\\\\u001b[0;32m--> 201\\\\u001b[0m     chunks \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[38;5;28;43mlist\\\\u001b[39;49m\\\\u001b[43m(\\\\u001b[49m\\\\u001b[43mchunks\\\\u001b[49m\\\\u001b[43m)\\\\u001b[49m\\\\n\\\\u001b[1;32m    202\\\\u001b[0m \\\\u001b[38;5;28;01mreturn\\\\u001b[39;00m \\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39mjoin(chunks)\\\\n\",\\n      \"File \\\\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/encoder.py:438\\\\u001b[0m, in \\\\u001b[0;36m_make_iterencode.<locals>._iterencode\\\\u001b[0;34m(o, _current_indent_level)\\\\u001b[0m\\\\n\\\\u001b[1;32m    436\\\\u001b[0m         \\\\u001b[38;5;28;01mraise\\\\u001b[39;00m \\\\u001b[38;5;167;01mValueError\\\\u001b[39;00m(\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m\\\\u001b[38;5;124mCircular reference detected\\\\u001b[39m\\\\u001b[38;5;124m\\\\\"\\\\u001b[39m)\\\\n\\\\u001b[1;32m    437\\\\u001b[0m     markers[markerid] \\\\u001b[38;5;241m=\\\\u001b[39m o\\\\n\\\\u001b[0;32m--> 438\\\\u001b[0m o \\\\u001b[38;5;241m=\\\\u001b[39m \\\\u001b[43m_default\\\\u001b[49m\\\\u001b[43m(\\\\u001b[49m\\\\u001b[43mo\\\\u001b[49m\\\\u001b[43m)\\\\u001b[49m\\\\n\\\\u001b[1;32m    439\\\\u001b[0m \\\\u001b[38;5;28;01myield from\\\\u001b[39;00m _iterencode(o, _current_indent_level)\\\\n\\\\u001b[1;32m    440\\\\u001b[0m \\\\u001b[38;5;28;01mif\\\\u001b[39;00m markers \\\\u001b[38;5;129;01mis\\\\u001b[39;00m \\\\u001b[38;5;129;01mnot\\\\u001b[39;00m \\\\u001b[38;5;28;01mNone\\\\u001b[39;00m:\\\\n\",\\n      \"File \\\\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/encoder.py:179\\\\u001b[0m, in \\\\u001b[0;36mJSONEncoder.default\\\\u001b[0;34m(self, o)\\\\u001b[0m\\\\n\\\\u001b[1;32m    160\\\\u001b[0m \\\\u001b[38;5;28;01mdef\\\\u001b[39;00m\\\\u001b[38;5;250m \\\\u001b[39m\\\\u001b[38;5;21mdefault\\\\u001b[39m(\\\\u001b[38;5;28mself\\\\u001b[39m, o):\\\\n\\\\u001b[1;32m    161\\\\u001b[0m \\\\u001b[38;5;250m    \\\\u001b[39m\\\\u001b[38;5;124;03m\\\\\"\\\\\"\\\\\"Implement this method in a subclass such that it returns\\\\u001b[39;00m\\\\n\\\\u001b[1;32m    162\\\\u001b[0m \\\\u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\\\\u001b[39;00m\\\\n\\\\u001b[1;32m    163\\\\u001b[0m \\\\u001b[38;5;124;03m    (to raise a ``TypeError``).\\\\u001b[39;00m\\\\n\\\\u001b[0;32m   (...)\\\\u001b[0m\\\\n\\\\u001b[1;32m    177\\\\u001b[0m \\\\n\\\\u001b[1;32m    178\\\\u001b[0m \\\\u001b[38;5;124;03m    \\\\\"\\\\\"\\\\\"\\\\u001b[39;00m\\\\n\\\\u001b[0;32m--> 179\\\\u001b[0m     \\\\u001b[38;5;28;01mraise\\\\u001b[39;00m \\\\u001b[38;5;167;01mTypeError\\\\u001b[39;00m(\\\\u001b[38;5;124mf\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mObject of type \\\\u001b[39m\\\\u001b[38;5;132;01m{\\\\u001b[39;00mo\\\\u001b[38;5;241m.\\\\u001b[39m\\\\u001b[38;5;18m__class__\\\\u001b[39m\\\\u001b[38;5;241m.\\\\u001b[39m\\\\u001b[38;5;18m__name__\\\\u001b[39m\\\\u001b[38;5;132;01m}\\\\u001b[39;00m\\\\u001b[38;5;124m \\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\n\\\\u001b[1;32m    180\\\\u001b[0m                     \\\\u001b[38;5;124mf\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m\\\\u001b[38;5;124mis not JSON serializable\\\\u001b[39m\\\\u001b[38;5;124m\\'\\\\u001b[39m)\\\\n\",\\n      \"\\\\u001b[0;31mTypeError\\\\u001b[0m: Object of type ModelResponse is not JSON serializable\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"import json\\\\n\",\\n    \"import os\\\\n\",\\n    \"from typing import List\\\\n\",\\n    \"\\\\n\",\\n    \"from litellm import completion\\\\n\",\\n    \"\\\\n\",\\n    \"def list_files() -> List[str]:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"List files in the current directory.\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    return os.listdir(\\\\\".\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"def read_file(file_name: str) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Read a file\\'s contents.\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    try:\\\\n\",\\n    \"        with open(file_name, \\\\\"r\\\\\") as file:\\\\n\",\\n    \"            return file.read()\\\\n\",\\n    \"    except FileNotFoundError:\\\\n\",\\n    \"        return f\\\\\"Error: {file_name} not found.\\\\\"\\\\n\",\\n    \"    except Exception as e:\\\\n\",\\n    \"        return f\\\\\"Error: {str(e)}\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"\\\\n\",\\n    \"tool_functions = {\\\\n\",\\n    \"    \\\\\"list_files\\\\\": list_files,\\\\n\",\\n    \"    \\\\\"read_file\\\\\": read_file\\\\n\",\\n    \"}\\\\n\",\\n    \"\\\\n\",\\n    \"tools = [\\\\n\",\\n    \"    {\\\\n\",\\n    \"        \\\\\"type\\\\\": \\\\\"function\\\\\",\\\\n\",\\n    \"        \\\\\"function\\\\\": {\\\\n\",\\n    \"            \\\\\"name\\\\\": \\\\\"list_files\\\\\",\\\\n\",\\n    \"            \\\\\"description\\\\\": \\\\\"Returns a list of files in the directory.\\\\\",\\\\n\",\\n    \"            \\\\\"parameters\\\\\": {\\\\\"type\\\\\": \\\\\"object\\\\\", \\\\\"properties\\\\\": {}, \\\\\"required\\\\\": []}\\\\n\",\\n    \"        }\\\\n\",\\n    \"    },\\\\n\",\\n    \"    {\\\\n\",\\n    \"        \\\\\"type\\\\\": \\\\\"function\\\\\",\\\\n\",\\n    \"        \\\\\"function\\\\\": {\\\\n\",\\n    \"            \\\\\"name\\\\\": \\\\\"read_file\\\\\",\\\\n\",\\n    \"            \\\\\"description\\\\\": \\\\\"Reads the content of a specified file in the directory.\\\\\",\\\\n\",\\n    \"            \\\\\"parameters\\\\\": {\\\\n\",\\n    \"                \\\\\"type\\\\\": \\\\\"object\\\\\",\\\\n\",\\n    \"                \\\\\"properties\\\\\": {\\\\\"file_name\\\\\": {\\\\\"type\\\\\": \\\\\"string\\\\\"}},\\\\n\",\\n    \"                \\\\\"required\\\\\": [\\\\\"file_name\\\\\"]\\\\n\",\\n    \"            }\\\\n\",\\n    \"        }\\\\n\",\\n    \"    }\\\\n\",\\n    \"]\\\\n\",\\n    \"\\\\n\",\\n    \"# Our rules are simplified since we don\\'t have to worry about getting a specific output format\\\\n\",\\n    \"agent_rules = [{\\\\n\",\\n    \"    \\\\\"role\\\\\": \\\\\"system\\\\\",\\\\n\",\\n    \"    \\\\\"content\\\\\": \\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"You are an AI agent that can perform tasks by using available tools.\\\\n\",\\n    \"\\\\n\",\\n    \"If a user asks about files, documents, or content, first list the files before reading them.\\\\n\",\\n    \"\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"}]\\\\n\",\\n    \"\\\\n\",\\n    \"user_task = input(\\\\\"What would you like me to do? \\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"memory = [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": user_task}]\\\\n\",\\n    \"\\\\n\",\\n    \"messages = agent_rules + memory\\\\n\",\\n    \"\\\\n\",\\n    \"response = completion(\\\\n\",\\n    \"    model=\\\\\"openai/gpt-4o\\\\\",\\\\n\",\\n    \"    messages=messages,\\\\n\",\\n    \"    tools=tools,\\\\n\",\\n    \"    max_tokens=1024\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"# Extract the tool call from the response, note we don\\'t have to parse now!\\\\n\",\\n    \"tool = response.choices[0].message.tool_calls[0]\\\\n\",\\n    \"tool_name = tool.function.name\\\\n\",\\n    \"tool_args = json.loads(tool.function.arguments)\\\\n\",\\n    \"result = tool_functions[tool_name](**tool_args)\\\\n\",\\n    \"\\\\n\",\\n    \"print(json.dumps(response.choices[0], indent=2))\\\\n\",\\n    \"os.environ[\\'OPENAI_API_KEY\\'] = api_ke\\\\n\",\\n    \"print(f\\\\\"Tool Name: {tool_name}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Tool Arguments: {tool_args}\\\\\")\\\\n\",\\n    \"print(f\\\\\"Result: {result}\\\\\")\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv (3.9.6)\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.6\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 5\\n}\\n'}\n",
      "Executing: read_file with args {'file_name': 'colab-sample.ipynb'}\n",
      "Result: {'result': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 1,\\n   \"id\": \"629ceda1\",\\n   \"metadata\": {\\n    \"vscode\": {\\n     \"languageId\": \"plaintext\"\\n    }\\n   },\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"86400\"\\n      ]\\n     },\\n     \"execution_count\": 1,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"seconds_in_day = 24 * 60 * 60\\\\n\",\\n    \"seconds_in_day\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"id\": \"02210274\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"# This is the heading.\\\\n\",\\n    \"I will push this file to the github remote, then open it in **colab**.\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"Python 3 (ipykernel)\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.6\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 5\\n}\\n'}\n",
      "Executing: read_file with args {'file_name': 'python-calls.ipynb'}\n",
      "Result: {'result': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 7,\\n   \"id\": \"8d1ac11b\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import os\\\\n\",\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv()\\\\n\",\\n    \"api_key = os.getenv(\\'OPENAPI_API_KEY\\')\\\\n\",\\n    \"os.environ[\\'OPENAI_API_KEY\\'] = api_key\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"id\": \"66afb5a9\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"Below code was copied from https://colab.research.google.com/drive/1W3LEOFjAQs69PJ3rM1aYG8Cofo_de6XH?usp=sharing&pli=1#scrollTo=Mwe2eeOQB0cC\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 8,\\n   \"id\": \"e2717b88\",\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"Agent thinking...\\\\n\",\\n      \"Agent response: To summarize the contents of the file named module-1-example, I first need to get a list of all the files in the current directory to identify if the file exists. \\\\n\",\\n      \"\\\\n\",\\n      \"```action\\\\n\",\\n      \"{\\\\n\",\\n      \"    \\\\\"tool_name\\\\\": \\\\\"list_files\\\\\",\\\\n\",\\n      \"    \\\\\"args\\\\\": {}\\\\n\",\\n      \"}\\\\n\",\\n      \"```\\\\n\",\\n      \"Action result: {\\'result\\': [\\'colab-sample.ipynb\\', \\'python-calls.ipynb\\', \\'.env\\', \\'module-1-example.ipynb\\', \\'.ipynb_checkpoints\\']}\\\\n\",\\n      \"Agent thinking...\\\\n\",\\n      \"Agent response: The file \\\\\"module-1-example.ipynb\\\\\" exists in the directory. I will now read the contents of this file to provide a summary.\\\\n\",\\n      \"\\\\n\",\\n      \"```action\\\\n\",\\n      \"{\\\\n\",\\n      \"    \\\\\"tool_name\\\\\": \\\\\"read_file\\\\\",\\\\n\",\\n      \"    \\\\\"args\\\\\": {\\\\\"file_name\\\\\": \\\\\"module-1-example.ipynb\\\\\"}\\\\n\",\\n      \"}\\\\n\",\\n      \"```\\\\n\",\\n      \"Action result: {\\'result\\': \\'{\\\\\\\\n \\\\\"cells\\\\\": [\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": 1,\\\\\\\\n   \\\\\"id\\\\\": \\\\\"6513f3e8\\\\\",\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"import os\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"from dotenv import load_dotenv\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"load_dotenv()\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"api_key = os.getenv(\\\\\\\\\\'OPENAPI_API_KEY\\\\\\\\\\')\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"os.environ[\\\\\\\\\\'OPENAI_API_KEY\\\\\\\\\\'] = api_key\\\\\"\\\\\\\\n   ]\\\\\\\\n  },\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": null,\\\\\\\\n   \\\\\"id\\\\\": \\\\\"6d4d9be5\\\\\",\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"from litellm import completion\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"from litellm import _turn_on_debug\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"from typing import List, Dict\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"import json\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"# _turn_on_debug()\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"def generate_response(messages: List[Dict]) -> str:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    \\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"Call LLM to get response\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    response = completion(\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        model=\\\\\\\\\\\\\\\\\\\\\"openai/gpt-4o\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        messages=messages,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        max_tokens=1024\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    )\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    return response.choices[0].message.content\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"def print_conversation(messages: List[Dict], response: str) -> str:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    \\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"Print the conversation in a readable format\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    response = generate_response(messages)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print(\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--> Prompt:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\') \\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print(json.dumps(messages, indent=2))\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print(\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--> Response:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\')\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print(response)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    return response\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"messages = [\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   {\\\\\\\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"system\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"You are an expert software engineer that prefers functional programming.\\\\\\\\\\\\\\\\\\\\\"},\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   {\\\\\\\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"Write a function to swap the keys and values in a dictionary.\\\\\\\\\\\\\\\\\\\\\"}\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"]\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"response = generate_response(messages)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"print_conversation(messages, response)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"# We are going to make this verbose so it is clear what\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"# is going on. In a real application, you would likely\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"# just append to the messages list.\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"messages = [\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   {\\\\\\\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"system\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"You are an expert software engineer that prefers functional programming.\\\\\\\\\\\\\\\\\\\\\"},\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   {\\\\\\\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"Write a function to swap the keys and values in a dictionary.\\\\\\\\\\\\\\\\\\\\\"},\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   \\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   # Here is the assistant\\\\\\\\\\'s response from the previous step\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   # with the code. This gives it \\\\\\\\\\\\\\\\\\\\\"memory\\\\\\\\\\\\\\\\\\\\\" of the previous\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   # interaction.\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   {\\\\\\\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"assistant\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\\\\\\\": response},\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   \\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   # Now, we can ask the assistant to update the function\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"   {\\\\\\\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"Update the function to include documentation.\\\\\\\\\\\\\\\\\\\\\"}\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"]\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"response = generate_response(messages)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"print_conversation(messages, response)\\\\\"\\\\\\\\n   ]\\\\\\\\n  },\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"markdown\\\\\",\\\\\\\\n   \\\\\"id\\\\\": \\\\\"17f4f2e5\\\\\",\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"# Practice Exercise\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"This exercise will allow you to practice programmatically sending prompts to an LLM and managing memory.\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"For this exercise, you should write a program that uses sequential prompts to generate any Python function based on user input. The program should:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"First Prompt:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Ask the user what function they want to create\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Ask the LLM to write a basic Python function based on the user’s description\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Store the response for use in subsequent prompts\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Parse the response to separate the code from the commentary by the LLM\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Second Prompt:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Pass the code generated from the first prompt\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Ask the LLM to add comprehensive documentation including:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Function description\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Parameter descriptions\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Return value description\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Example usage\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Edge cases\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Third Prompt:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Pass the documented code generated from the second prompt\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Ask the LLM to add test cases using Python’s unittest framework\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Tests should cover:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Basic functionality\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Edge cases\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Error cases\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Various input scenarios\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Requirements:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Use the LiteLLM library\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Maintain conversation context between prompts\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Print each step of the development process\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"Save the final version to a Python file\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"If you want to practice further, try using the system message to force the LLM to always output code that has a specific style or uses particular libraries.\\\\\"\\\\\\\\n   ]\\\\\\\\n  },\\\\\\\\n  {\\\\\\\\n   \\\\\"cell_type\\\\\": \\\\\"code\\\\\",\\\\\\\\n   \\\\\"execution_count\\\\\": null,\\\\\\\\n   \\\\\"id\\\\\": \\\\\"cbcc9a4f\\\\\",\\\\\\\\n   \\\\\"metadata\\\\\": {},\\\\\\\\n   \\\\\"outputs\\\\\": [],\\\\\\\\n   \\\\\"source\\\\\": [\\\\\\\\n    \\\\\"from litellm import completion\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"from litellm import _turn_on_debug\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"from typing import List, Dict\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"import json\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"# _turn_on_debug()\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"def generate_response(messages: List[Dict]) -> str:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    \\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"Call LLM to get response\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    response = completion(\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        model=\\\\\\\\\\\\\\\\\\\\\"openai/gpt-4o\\\\\\\\\\\\\\\\\\\\\",\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        messages=messages,\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"        max_tokens=1024\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    )\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    return response.choices[0].message.content\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"def print_conversation(messages: List[Dict], response: str) -> str:\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    \\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"Print the conversation in a readable format\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    response = generate_response(messages)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print(\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--> Prompt:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\') \\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print(json.dumps(messages, indent=2))\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print(\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--> Response:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\')\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    print(response)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"    return response\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"messages = [];\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"responses = [];\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"messages.append( {\\\\\\\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"system\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"You are an expert software engineer that prefers functional programming.\\\\\\\\\\\\\\\\\\\\\"})\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"user_function_request_str = input(\\\\\\\\\\\\\\\\\\\\\"What kind of function would you like to create? \\\\\\\\\\\\\\\\\\\\\")\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"messages.append( {\\\\\\\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\\\\\\\": f\\\\\\\\\\\\\\\\\\\\\"Write a function for a customer, given the following requirement: {user_function_request_str}\\\\\\\\\\\\\\\\\\\\\"} )\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"response = generate_response(messages)\\\\\\\\\\\\\\\\n\\\\\",\\\\\\\\n    \\\\\"print_conversation(messages, response)\\\\\"\\\\\\\\n   ]\\\\\\\\n  }\\\\\\\\n ],\\\\\\\\n \\\\\"metadata\\\\\": {\\\\\\\\n  \\\\\"kernelspec\\\\\": {\\\\\\\\n   \\\\\"display_name\\\\\": \\\\\"venv (3.9.6)\\\\\",\\\\\\\\n   \\\\\"language\\\\\": \\\\\"python\\\\\",\\\\\\\\n   \\\\\"name\\\\\": \\\\\"python3\\\\\"\\\\\\\\n  },\\\\\\\\n  \\\\\"language_info\\\\\": {\\\\\\\\n   \\\\\"codemirror_mode\\\\\": {\\\\\\\\n    \\\\\"name\\\\\": \\\\\"ipython\\\\\",\\\\\\\\n    \\\\\"version\\\\\": 3\\\\\\\\n   },\\\\\\\\n   \\\\\"file_extension\\\\\": \\\\\".py\\\\\",\\\\\\\\n   \\\\\"mimetype\\\\\": \\\\\"text/x-python\\\\\",\\\\\\\\n   \\\\\"name\\\\\": \\\\\"python\\\\\",\\\\\\\\n   \\\\\"nbconvert_exporter\\\\\": \\\\\"python\\\\\",\\\\\\\\n   \\\\\"pygments_lexer\\\\\": \\\\\"ipython3\\\\\",\\\\\\\\n   \\\\\"version\\\\\": \\\\\"3.9.6\\\\\"\\\\\\\\n  }\\\\\\\\n },\\\\\\\\n \\\\\"nbformat\\\\\": 4,\\\\\\\\n \\\\\"nbformat_minor\\\\\": 5\\\\\\\\n}\\\\\\\\n\\'}\\\\n\",\\n      \"Agent thinking...\\\\n\",\\n      \"Agent response: The file \\\\\"module-1-example.ipynb\\\\\" is a Jupyter notebook that demonstrates the use of the LiteLLM library to generate Python functions through sequential prompts to a language model. Below is a summary of the contents:\\\\n\",\\n      \"\\\\n\",\\n      \"1. **Environment Setup**:\\\\n\",\\n      \"   - The notebook begins by importing necessary libraries and setting up an API key from an environment variable.\\\\n\",\\n      \"\\\\n\",\\n      \"2. **Function Definitions**:\\\\n\",\\n      \"   - `generate_response()`: Calls a language model to generate a response given a list of messages.\\\\n\",\\n      \"   - `print_conversation()`: Prints the conversation in a readable format, displaying both the prompts and the model\\'s responses.\\\\n\",\\n      \"  \\\\n\",\\n      \"3. **Exercise Description**:\\\\n\",\\n      \"   - The notebook includes a markdown cell describing a practice exercise to implement a Python program that interacts with a language model to iteratively refine a Python function.\\\\n\",\\n      \"   - The process includes getting an initial function description from the user, asking for code generation, adding documentation, and creating test cases using Python\\'s `unittest` framework.\\\\n\",\\n      \"\\\\n\",\\n      \"4. **Sequential Prompts**:\\\\n\",\\n      \"   - The notebook shows how the conversation with the model can be maintained by appending previous interactions to the message history, allowing the model to build upon prior context.\\\\n\",\\n      \"\\\\n\",\\n      \"5. **Iterative Coding**:\\\\n\",\\n      \"   - Users are guided to input their desired function, after which prompts are generated to first create basic code, then improve it with documentation, and finally generate appropriate test cases.\\\\n\",\\n      \"\\\\n\",\\n      \"This example illustrates how to structure AI-assisted programming tasks in notebooks using Python and the LiteLLM library efficiently. \\\\n\",\\n      \"\\\\n\",\\n      \"```action\\\\n\",\\n      \"{\\\\n\",\\n      \"    \\\\\"tool_name\\\\\": \\\\\"terminate\\\\\",\\\\n\",\\n      \"    \\\\\"args\\\\\": {\\\\n\",\\n      \"        \\\\\"message\\\\\": \\\\\"The \\'module-1-example.ipynb\\' file demonstrates using the LiteLLM library to generate Python function through iterative language model prompts, featuring function generation, documentation, and test case creation.\\\\\"\\\\n\",\\n      \"    }\\\\n\",\\n      \"}\\\\n\",\\n      \"```\\\\n\",\\n      \"The \\'module-1-example.ipynb\\' file demonstrates using the LiteLLM library to generate Python function through iterative language model prompts, featuring function generation, documentation, and test case creation.\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"import json\\\\n\",\\n    \"import os\\\\n\",\\n    \"import sys\\\\n\",\\n    \"from litellm import completion\\\\n\",\\n    \"from typing import List, Dict\\\\n\",\\n    \"\\\\n\",\\n    \"def extract_markdown_block(response: str, block_type: str = \\\\\"json\\\\\") -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Extract code block from response\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"    if not \\'```\\' in response:\\\\n\",\\n    \"        return response\\\\n\",\\n    \"\\\\n\",\\n    \"    code_block = response.split(\\'```\\')[1].strip()\\\\n\",\\n    \"\\\\n\",\\n    \"    if code_block.startswith(block_type):\\\\n\",\\n    \"        code_block = code_block[len(block_type):].strip()\\\\n\",\\n    \"\\\\n\",\\n    \"    return code_block\\\\n\",\\n    \"\\\\n\",\\n    \"def generate_response(messages: List[Dict]) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Call LLM to get a response.\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    response = completion(\\\\n\",\\n    \"        model=\\\\\"openai/gpt-4o\\\\\",\\\\n\",\\n    \"        messages=messages,\\\\n\",\\n    \"        max_tokens=1024\\\\n\",\\n    \"    )\\\\n\",\\n    \"    return response.choices[0].message.content.strip()\\\\n\",\\n    \"\\\\n\",\\n    \"def parse_action(response: str) -> Dict:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Parse the LLM response into a structured action dictionary.\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    try:\\\\n\",\\n    \"        response = extract_markdown_block(response, \\\\\"action\\\\\")\\\\n\",\\n    \"        response_json = json.loads(response)\\\\n\",\\n    \"        if \\\\\"tool_name\\\\\" in response_json and \\\\\"args\\\\\" in response_json:\\\\n\",\\n    \"            return response_json\\\\n\",\\n    \"        else:\\\\n\",\\n    \"            return {\\\\\"tool_name\\\\\": \\\\\"error\\\\\", \\\\\"args\\\\\": {\\\\\"message\\\\\": \\\\\"You must respond with a JSON tool invocation.\\\\\"}}\\\\n\",\\n    \"    except json.JSONDecodeError:\\\\n\",\\n    \"        return {\\\\\"tool_name\\\\\": \\\\\"error\\\\\", \\\\\"args\\\\\": {\\\\\"message\\\\\": \\\\\"Invalid JSON response. You must respond with a JSON tool invocation.\\\\\"}}\\\\n\",\\n    \"\\\\n\",\\n    \"def list_files() -> List[str]:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"List files in the current directory.\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    return os.listdir(\\\\\".\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"def read_file(file_name: str) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Read a file\\'s contents.\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    try:\\\\n\",\\n    \"        with open(file_name, \\\\\"r\\\\\") as file:\\\\n\",\\n    \"            return file.read()\\\\n\",\\n    \"    except FileNotFoundError:\\\\n\",\\n    \"        return f\\\\\"Error: {file_name} not found.\\\\\"\\\\n\",\\n    \"    except Exception as e:\\\\n\",\\n    \"        return f\\\\\"Error: {str(e)}\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"# Define system instructions (Agent Rules)\\\\n\",\\n    \"agent_rules = [{\\\\n\",\\n    \"    \\\\\"role\\\\\": \\\\\"system\\\\\",\\\\n\",\\n    \"    \\\\\"content\\\\\": \\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"You are an AI agent that can perform tasks by using available tools.\\\\n\",\\n    \"\\\\n\",\\n    \"Available tools:\\\\n\",\\n    \"\\\\n\",\\n    \"```json\\\\n\",\\n    \"{\\\\n\",\\n    \"    \\\\\"list_files\\\\\": {\\\\n\",\\n    \"        \\\\\"description\\\\\": \\\\\"Lists all files in the current directory.\\\\\",\\\\n\",\\n    \"        \\\\\"parameters\\\\\": {}\\\\n\",\\n    \"    },\\\\n\",\\n    \"    \\\\\"read_file\\\\\": {\\\\n\",\\n    \"        \\\\\"description\\\\\": \\\\\"Reads the content of a file.\\\\\",\\\\n\",\\n    \"        \\\\\"parameters\\\\\": {\\\\n\",\\n    \"            \\\\\"file_name\\\\\": {\\\\n\",\\n    \"                \\\\\"type\\\\\": \\\\\"string\\\\\",\\\\n\",\\n    \"                \\\\\"description\\\\\": \\\\\"The name of the file to read.\\\\\"\\\\n\",\\n    \"            }\\\\n\",\\n    \"        }\\\\n\",\\n    \"    },\\\\n\",\\n    \"    \\\\\"terminate\\\\\": {\\\\n\",\\n    \"        \\\\\"description\\\\\": \\\\\"Ends the agent loop and provides a summary of the task.\\\\\",\\\\n\",\\n    \"        \\\\\"parameters\\\\\": {\\\\n\",\\n    \"            \\\\\"message\\\\\": {\\\\n\",\\n    \"                \\\\\"type\\\\\": \\\\\"string\\\\\",\\\\n\",\\n    \"                \\\\\"description\\\\\": \\\\\"Summary message to return to the user.\\\\\"\\\\n\",\\n    \"            }\\\\n\",\\n    \"        }\\\\n\",\\n    \"    }\\\\n\",\\n    \"}\\\\n\",\\n    \"```\\\\n\",\\n    \"\\\\n\",\\n    \"If a user asks about files, documents, or content, first list the files before reading them.\\\\n\",\\n    \"\\\\n\",\\n    \"When you are done, terminate the conversation by using the \\\\\"terminate\\\\\" tool and I will provide the results to the user.\\\\n\",\\n    \"\\\\n\",\\n    \"Important!!! Every response MUST have an action.\\\\n\",\\n    \"You must ALWAYS respond in this format:\\\\n\",\\n    \"\\\\n\",\\n    \"<Stop and think step by step. Parameters map to args. Insert a rich description of your step by step thoughts here.>\\\\n\",\\n    \"\\\\n\",\\n    \"```action\\\\n\",\\n    \"{\\\\n\",\\n    \"    \\\\\"tool_name\\\\\": \\\\\"insert tool_name\\\\\",\\\\n\",\\n    \"    \\\\\"args\\\\\": {...fill in any required arguments here...}\\\\n\",\\n    \"}\\\\n\",\\n    \"```\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"}]\\\\n\",\\n    \"\\\\n\",\\n    \"# Initialize agent parameters\\\\n\",\\n    \"iterations = 0\\\\n\",\\n    \"max_iterations = 10\\\\n\",\\n    \"\\\\n\",\\n    \"user_task = input(\\\\\"What would you like me to do? \\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"memory = [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": user_task}]\\\\n\",\\n    \"\\\\n\",\\n    \"# The Agent Loop\\\\n\",\\n    \"while iterations < max_iterations:\\\\n\",\\n    \"    # 1. Construct prompt: Combine agent rules with memory\\\\n\",\\n    \"    prompt = agent_rules + memory\\\\n\",\\n    \"\\\\n\",\\n    \"    # 2. Generate response from LLM\\\\n\",\\n    \"    print(\\\\\"Agent thinking...\\\\\")\\\\n\",\\n    \"    response = generate_response(prompt)\\\\n\",\\n    \"    print(f\\\\\"Agent response: {response}\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"    # 3. Parse response to determine action\\\\n\",\\n    \"    action = parse_action(response)\\\\n\",\\n    \"    result = \\\\\"Action executed\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"    if action[\\\\\"tool_name\\\\\"] == \\\\\"list_files\\\\\":\\\\n\",\\n    \"        result = {\\\\\"result\\\\\": list_files()}\\\\n\",\\n    \"    elif action[\\\\\"tool_name\\\\\"] == \\\\\"read_file\\\\\":\\\\n\",\\n    \"        result = {\\\\\"result\\\\\": read_file(action[\\\\\"args\\\\\"][\\\\\"file_name\\\\\"])}\\\\n\",\\n    \"    elif action[\\\\\"tool_name\\\\\"] == \\\\\"error\\\\\":\\\\n\",\\n    \"        result = {\\\\\"error\\\\\": action[\\\\\"args\\\\\"][\\\\\"message\\\\\"]}\\\\n\",\\n    \"    elif action[\\\\\"tool_name\\\\\"] == \\\\\"terminate\\\\\":\\\\n\",\\n    \"        print(action[\\\\\"args\\\\\"][\\\\\"message\\\\\"])\\\\n\",\\n    \"        break\\\\n\",\\n    \"    else:\\\\n\",\\n    \"        result = {\\\\\"error\\\\\": \\\\\"Unknown action: \\\\\" + action[\\\\\"tool_name\\\\\"]}\\\\n\",\\n    \"\\\\n\",\\n    \"    print(f\\\\\"Action result: {result}\\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"    # 5. Update memory with response and results\\\\n\",\\n    \"    memory.extend([\\\\n\",\\n    \"        {\\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": response},\\\\n\",\\n    \"        {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": json.dumps(result)}\\\\n\",\\n    \"    ])\\\\n\",\\n    \"\\\\n\",\\n    \"    # 6. Check termination condition\\\\n\",\\n    \"    if action[\\\\\"tool_name\\\\\"] == \\\\\"terminate\\\\\":\\\\n\",\\n    \"        break\\\\n\",\\n    \"\\\\n\",\\n    \"    iterations += 1\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv (3.9.6)\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.6\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 5\\n}\\n'}\n",
      "Executing: read_file with args {'file_name': 'module-1-example.ipynb'}\n",
      "Result: {'result': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 1,\\n   \"id\": \"6513f3e8\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import os\\\\n\",\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv()\\\\n\",\\n    \"api_key = os.getenv(\\'OPENAPI_API_KEY\\')\\\\n\",\\n    \"os.environ[\\'OPENAI_API_KEY\\'] = api_key\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"id\": \"6d4d9be5\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from litellm import completion\\\\n\",\\n    \"from litellm import _turn_on_debug\\\\n\",\\n    \"from typing import List, Dict\\\\n\",\\n    \"import json\\\\n\",\\n    \"\\\\n\",\\n    \"# _turn_on_debug()\\\\n\",\\n    \"\\\\n\",\\n    \"def generate_response(messages: List[Dict]) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Call LLM to get response\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    response = completion(\\\\n\",\\n    \"        model=\\\\\"openai/gpt-4o\\\\\",\\\\n\",\\n    \"        messages=messages,\\\\n\",\\n    \"        max_tokens=1024\\\\n\",\\n    \"    )\\\\n\",\\n    \"    return response.choices[0].message.content\\\\n\",\\n    \"\\\\n\",\\n    \"def print_conversation(messages: List[Dict], response: str) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Print the conversation in a readable format\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    response = generate_response(messages)\\\\n\",\\n    \"    print(\\'\\\\\\\\n--> Prompt:\\\\\\\\n\\') \\\\n\",\\n    \"    print(json.dumps(messages, indent=2))\\\\n\",\\n    \"    print(\\'\\\\\\\\n--> Response:\\\\\\\\n\\')\\\\n\",\\n    \"    print(response)\\\\n\",\\n    \"    return response\\\\n\",\\n    \"\\\\n\",\\n    \"messages = [\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": \\\\\"You are an expert software engineer that prefers functional programming.\\\\\"},\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Write a function to swap the keys and values in a dictionary.\\\\\"}\\\\n\",\\n    \"]\\\\n\",\\n    \"\\\\n\",\\n    \"response = generate_response(messages)\\\\n\",\\n    \"print_conversation(messages, response)\\\\n\",\\n    \"\\\\n\",\\n    \"# We are going to make this verbose so it is clear what\\\\n\",\\n    \"# is going on. In a real application, you would likely\\\\n\",\\n    \"# just append to the messages list.\\\\n\",\\n    \"messages = [\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": \\\\\"You are an expert software engineer that prefers functional programming.\\\\\"},\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Write a function to swap the keys and values in a dictionary.\\\\\"},\\\\n\",\\n    \"   \\\\n\",\\n    \"   # Here is the assistant\\'s response from the previous step\\\\n\",\\n    \"   # with the code. This gives it \\\\\"memory\\\\\" of the previous\\\\n\",\\n    \"   # interaction.\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": response},\\\\n\",\\n    \"   \\\\n\",\\n    \"   # Now, we can ask the assistant to update the function\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Update the function to include documentation.\\\\\"}\\\\n\",\\n    \"]\\\\n\",\\n    \"\\\\n\",\\n    \"response = generate_response(messages)\\\\n\",\\n    \"print_conversation(messages, response)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"id\": \"17f4f2e5\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"# Practice Exercise\\\\n\",\\n    \"This exercise will allow you to practice programmatically sending prompts to an LLM and managing memory.\\\\n\",\\n    \"\\\\n\",\\n    \"For this exercise, you should write a program that uses sequential prompts to generate any Python function based on user input. The program should:\\\\n\",\\n    \"\\\\n\",\\n    \"First Prompt:\\\\n\",\\n    \"\\\\n\",\\n    \"Ask the user what function they want to create\\\\n\",\\n    \"Ask the LLM to write a basic Python function based on the user’s description\\\\n\",\\n    \"Store the response for use in subsequent prompts\\\\n\",\\n    \"Parse the response to separate the code from the commentary by the LLM\\\\n\",\\n    \"Second Prompt:\\\\n\",\\n    \"\\\\n\",\\n    \"Pass the code generated from the first prompt\\\\n\",\\n    \"Ask the LLM to add comprehensive documentation including:\\\\n\",\\n    \"Function description\\\\n\",\\n    \"Parameter descriptions\\\\n\",\\n    \"Return value description\\\\n\",\\n    \"Example usage\\\\n\",\\n    \"Edge cases\\\\n\",\\n    \"Third Prompt:\\\\n\",\\n    \"\\\\n\",\\n    \"Pass the documented code generated from the second prompt\\\\n\",\\n    \"Ask the LLM to add test cases using Python’s unittest framework\\\\n\",\\n    \"Tests should cover:\\\\n\",\\n    \"Basic functionality\\\\n\",\\n    \"Edge cases\\\\n\",\\n    \"Error cases\\\\n\",\\n    \"Various input scenarios\\\\n\",\\n    \"Requirements:\\\\n\",\\n    \"\\\\n\",\\n    \"Use the LiteLLM library\\\\n\",\\n    \"Maintain conversation context between prompts\\\\n\",\\n    \"Print each step of the development process\\\\n\",\\n    \"Save the final version to a Python file\\\\n\",\\n    \"If you want to practice further, try using the system message to force the LLM to always output code that has a specific style or uses particular libraries.\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"id\": \"cbcc9a4f\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from litellm import completion\\\\n\",\\n    \"from litellm import _turn_on_debug\\\\n\",\\n    \"from typing import List, Dict\\\\n\",\\n    \"import json\\\\n\",\\n    \"\\\\n\",\\n    \"# _turn_on_debug()\\\\n\",\\n    \"\\\\n\",\\n    \"def generate_response(messages: List[Dict]) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Call LLM to get response\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    response = completion(\\\\n\",\\n    \"        model=\\\\\"openai/gpt-4o\\\\\",\\\\n\",\\n    \"        messages=messages,\\\\n\",\\n    \"        max_tokens=1024\\\\n\",\\n    \"    )\\\\n\",\\n    \"    return response.choices[0].message.content\\\\n\",\\n    \"\\\\n\",\\n    \"def print_conversation(messages: List[Dict], response: str) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Print the conversation in a readable format\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    response = generate_response(messages)\\\\n\",\\n    \"    print(\\'\\\\\\\\n--> Prompt:\\\\\\\\n\\') \\\\n\",\\n    \"    print(json.dumps(messages, indent=2))\\\\n\",\\n    \"    print(\\'\\\\\\\\n--> Response:\\\\\\\\n\\')\\\\n\",\\n    \"    print(response)\\\\n\",\\n    \"    return response\\\\n\",\\n    \"\\\\n\",\\n    \"messages = [];\\\\n\",\\n    \"responses = [];\\\\n\",\\n    \"messages.append( {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": \\\\\"You are an expert software engineer that prefers functional programming.\\\\\"})\\\\n\",\\n    \"\\\\n\",\\n    \"user_function_request_str = input(\\\\\"What kind of function would you like to create? \\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"messages.append( {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": f\\\\\"Write a function for a customer, given the following requirement: {user_function_request_str}\\\\\"} )\\\\n\",\\n    \"\\\\n\",\\n    \"response = generate_response(messages)\\\\n\",\\n    \"print_conversation(messages, response)\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv (3.9.6)\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.6\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 5\\n}\\n'}\n",
      "Termination message: The biggest file in the current folder is `function-apis.ipynb`.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "from litellm import completion\n",
    "\n",
    "def list_files() -> List[str]:\n",
    "    \"\"\"List files in the current directory.\"\"\"\n",
    "    return os.listdir(\".\")\n",
    "\n",
    "def read_file(file_name: str) -> str:\n",
    "    \"\"\"Read a file's contents.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: {file_name} not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def terminate(message: str) -> None:\n",
    "    \"\"\"Terminate the agent loop and provide a summary message.\"\"\"\n",
    "    print(f\"Termination message: {message}\")\n",
    "\n",
    "tool_functions = {\n",
    "    \"list_files\": list_files,\n",
    "    \"read_file\": read_file,\n",
    "    \"terminate\": terminate\n",
    "}\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"list_files\",\n",
    "            \"description\": \"Returns a list of files in the directory.\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"read_file\",\n",
    "            \"description\": \"Reads the content of a specified file in the directory.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"file_name\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"file_name\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"terminate\",\n",
    "            \"description\": \"Terminates the conversation. No further actions or interactions are possible after this. Prints the provided message for the user.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"message\": {\"type\": \"string\"},\n",
    "                },\n",
    "                \"required\": [\"message\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "agent_rules = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "You are an AI agent that can perform tasks by using available tools.\n",
    "\n",
    "If a user asks about files, documents, or content, first list the files before reading them.\n",
    "\n",
    "When you are done, terminate the conversation by using the \"terminate\" tool and I will provide the results to the user.\n",
    "\"\"\"\n",
    "}]\n",
    "\n",
    "# Initialize agent parameters\n",
    "iterations = 0\n",
    "max_iterations = 10\n",
    "\n",
    "user_task = input(\"What would you like me to do? \")\n",
    "\n",
    "memory = [{\"role\": \"user\", \"content\": user_task}]\n",
    "\n",
    "# The Agent Loop\n",
    "while iterations < max_iterations:\n",
    "\n",
    "    messages = agent_rules + memory\n",
    "\n",
    "    response = completion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "\n",
    "    if response.choices[0].message.tool_calls:\n",
    "        tool = response.choices[0].message.tool_calls[0]\n",
    "        tool_name = tool.function.name\n",
    "        tool_args = json.loads(tool.function.arguments)\n",
    "\n",
    "        action = {\n",
    "            \"tool_name\": tool_name,\n",
    "            \"args\": tool_args\n",
    "        }\n",
    "\n",
    "        if tool_name == \"terminate\":\n",
    "            print(f\"Termination message: {tool_args['message']}\")\n",
    "            break\n",
    "        elif tool_name in tool_functions:\n",
    "            try:\n",
    "                result = {\"result\": tool_functions[tool_name](**tool_args)}\n",
    "            except Exception as e:\n",
    "                result = {\"error\":f\"Error executing {tool_name}: {str(e)}\"}\n",
    "        else:\n",
    "            result = {\"error\": f\"Unknown tool: {tool_name}\"}\n",
    "\n",
    "        print(f\"Executing: {tool_name} with args {tool_args}\")\n",
    "        print(f\"Result: {result}\")\n",
    "        memory.extend([\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps(action)},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(result)}\n",
    "        ])\n",
    "    else:\n",
    "        result = response.choices[0].message.content\n",
    "        print(f\"Response: {result}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
