{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6513f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAPI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "from litellm import _turn_on_debug\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "# _turn_on_debug()\n",
    "\n",
    "def generate_response(messages: List[Dict]) -> str:\n",
    "    \"\"\"Call LLM to get response\"\"\"\n",
    "    response = completion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def print_conversation(messages: List[Dict], response: str) -> str:\n",
    "    \"\"\"Print the conversation in a readable format\"\"\"\n",
    "    response = generate_response(messages)\n",
    "    print('\\n--> Prompt:\\n') \n",
    "    print(json.dumps(messages, indent=2))\n",
    "    print('\\n--> Response:\\n')\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "messages = [\n",
    "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
    "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
    "]\n",
    "\n",
    "response = generate_response(messages)\n",
    "print_conversation(messages, response)\n",
    "\n",
    "# We are going to make this verbose so it is clear what\n",
    "# is going on. In a real application, you would likely\n",
    "# just append to the messages list.\n",
    "messages = [\n",
    "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
    "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"},\n",
    "   \n",
    "   # Here is the assistant's response from the previous step\n",
    "   # with the code. This gives it \"memory\" of the previous\n",
    "   # interaction.\n",
    "   {\"role\": \"assistant\", \"content\": response},\n",
    "   \n",
    "   # Now, we can ask the assistant to update the function\n",
    "   {\"role\": \"user\", \"content\": \"Update the function to include documentation.\"}\n",
    "]\n",
    "\n",
    "response = generate_response(messages)\n",
    "print_conversation(messages, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4f2e5",
   "metadata": {},
   "source": [
    "# Practice Exercise\n",
    "This exercise will allow you to practice programmatically sending prompts to an LLM and managing memory.\n",
    "\n",
    "For this exercise, you should write a program that uses sequential prompts to generate any Python function based on user input. The program should:\n",
    "\n",
    "First Prompt:\n",
    "\n",
    "Ask the user what function they want to create\n",
    "Ask the LLM to write a basic Python function based on the user’s description\n",
    "Store the response for use in subsequent prompts\n",
    "Parse the response to separate the code from the commentary by the LLM\n",
    "Second Prompt:\n",
    "\n",
    "Pass the code generated from the first prompt\n",
    "Ask the LLM to add comprehensive documentation including:\n",
    "Function description\n",
    "Parameter descriptions\n",
    "Return value description\n",
    "Example usage\n",
    "Edge cases\n",
    "Third Prompt:\n",
    "\n",
    "Pass the documented code generated from the second prompt\n",
    "Ask the LLM to add test cases using Python’s unittest framework\n",
    "Tests should cover:\n",
    "Basic functionality\n",
    "Edge cases\n",
    "Error cases\n",
    "Various input scenarios\n",
    "Requirements:\n",
    "\n",
    "Use the LiteLLM library\n",
    "Maintain conversation context between prompts\n",
    "Print each step of the development process\n",
    "Save the final version to a Python file\n",
    "If you want to practice further, try using the system message to force the LLM to always output code that has a specific style or uses particular libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "from litellm import _turn_on_debug\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "# _turn_on_debug()\n",
    "\n",
    "def generate_response(messages: List[Dict]) -> str:\n",
    "    \"\"\"Call LLM to get response\"\"\"\n",
    "    response = completion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def print_conversation(messages: List[Dict], response: str) -> str:\n",
    "    \"\"\"Print the conversation in a readable format\"\"\"\n",
    "    response = generate_response(messages)\n",
    "    print('\\n--> Prompt:\\n') \n",
    "    print(json.dumps(messages, indent=2))\n",
    "    print('\\n--> Response:\\n')\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "messages = [];\n",
    "responses = [];\n",
    "messages.append( {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"})\n",
    "\n",
    "user_function_request_str = input(\"What kind of function would you like to create? \")\n",
    "\n",
    "messages.append( {\"role\": \"user\", \"content\": f\"Write a function for a customer, given the following requirement: {user_function_request_str}\"} )\n",
    "\n",
    "response = generate_response(messages)\n",
    "print_conversation(messages, response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
