{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d1ac11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAPI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66afb5a9",
   "metadata": {},
   "source": [
    "Below code was copied from https://colab.research.google.com/drive/1W3LEOFjAQs69PJ3rM1aYG8Cofo_de6XH?usp=sharing&pli=1#scrollTo=Mwe2eeOQB0cC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2717b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent thinking...\n",
      "Agent response: To summarize the contents of the file named module-1-example, I first need to get a list of all the files in the current directory to identify if the file exists. \n",
      "\n",
      "```action\n",
      "{\n",
      "    \"tool_name\": \"list_files\",\n",
      "    \"args\": {}\n",
      "}\n",
      "```\n",
      "Action result: {'result': ['colab-sample.ipynb', 'python-calls.ipynb', '.env', 'module-1-example.ipynb', '.ipynb_checkpoints']}\n",
      "Agent thinking...\n",
      "Agent response: The file \"module-1-example.ipynb\" exists in the directory. I will now read the contents of this file to provide a summary.\n",
      "\n",
      "```action\n",
      "{\n",
      "    \"tool_name\": \"read_file\",\n",
      "    \"args\": {\"file_name\": \"module-1-example.ipynb\"}\n",
      "}\n",
      "```\n",
      "Action result: {'result': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 1,\\n   \"id\": \"6513f3e8\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"import os\\\\n\",\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"\\\\n\",\\n    \"load_dotenv()\\\\n\",\\n    \"api_key = os.getenv(\\'OPENAPI_API_KEY\\')\\\\n\",\\n    \"os.environ[\\'OPENAI_API_KEY\\'] = api_key\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"id\": \"6d4d9be5\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from litellm import completion\\\\n\",\\n    \"from litellm import _turn_on_debug\\\\n\",\\n    \"from typing import List, Dict\\\\n\",\\n    \"import json\\\\n\",\\n    \"\\\\n\",\\n    \"# _turn_on_debug()\\\\n\",\\n    \"\\\\n\",\\n    \"def generate_response(messages: List[Dict]) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Call LLM to get response\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    response = completion(\\\\n\",\\n    \"        model=\\\\\"openai/gpt-4o\\\\\",\\\\n\",\\n    \"        messages=messages,\\\\n\",\\n    \"        max_tokens=1024\\\\n\",\\n    \"    )\\\\n\",\\n    \"    return response.choices[0].message.content\\\\n\",\\n    \"\\\\n\",\\n    \"def print_conversation(messages: List[Dict], response: str) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Print the conversation in a readable format\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    response = generate_response(messages)\\\\n\",\\n    \"    print(\\'\\\\\\\\n--> Prompt:\\\\\\\\n\\') \\\\n\",\\n    \"    print(json.dumps(messages, indent=2))\\\\n\",\\n    \"    print(\\'\\\\\\\\n--> Response:\\\\\\\\n\\')\\\\n\",\\n    \"    print(response)\\\\n\",\\n    \"    return response\\\\n\",\\n    \"\\\\n\",\\n    \"messages = [\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": \\\\\"You are an expert software engineer that prefers functional programming.\\\\\"},\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Write a function to swap the keys and values in a dictionary.\\\\\"}\\\\n\",\\n    \"]\\\\n\",\\n    \"\\\\n\",\\n    \"response = generate_response(messages)\\\\n\",\\n    \"print_conversation(messages, response)\\\\n\",\\n    \"\\\\n\",\\n    \"# We are going to make this verbose so it is clear what\\\\n\",\\n    \"# is going on. In a real application, you would likely\\\\n\",\\n    \"# just append to the messages list.\\\\n\",\\n    \"messages = [\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": \\\\\"You are an expert software engineer that prefers functional programming.\\\\\"},\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Write a function to swap the keys and values in a dictionary.\\\\\"},\\\\n\",\\n    \"   \\\\n\",\\n    \"   # Here is the assistant\\'s response from the previous step\\\\n\",\\n    \"   # with the code. This gives it \\\\\"memory\\\\\" of the previous\\\\n\",\\n    \"   # interaction.\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": response},\\\\n\",\\n    \"   \\\\n\",\\n    \"   # Now, we can ask the assistant to update the function\\\\n\",\\n    \"   {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Update the function to include documentation.\\\\\"}\\\\n\",\\n    \"]\\\\n\",\\n    \"\\\\n\",\\n    \"response = generate_response(messages)\\\\n\",\\n    \"print_conversation(messages, response)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"id\": \"17f4f2e5\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"# Practice Exercise\\\\n\",\\n    \"This exercise will allow you to practice programmatically sending prompts to an LLM and managing memory.\\\\n\",\\n    \"\\\\n\",\\n    \"For this exercise, you should write a program that uses sequential prompts to generate any Python function based on user input. The program should:\\\\n\",\\n    \"\\\\n\",\\n    \"First Prompt:\\\\n\",\\n    \"\\\\n\",\\n    \"Ask the user what function they want to create\\\\n\",\\n    \"Ask the LLM to write a basic Python function based on the user’s description\\\\n\",\\n    \"Store the response for use in subsequent prompts\\\\n\",\\n    \"Parse the response to separate the code from the commentary by the LLM\\\\n\",\\n    \"Second Prompt:\\\\n\",\\n    \"\\\\n\",\\n    \"Pass the code generated from the first prompt\\\\n\",\\n    \"Ask the LLM to add comprehensive documentation including:\\\\n\",\\n    \"Function description\\\\n\",\\n    \"Parameter descriptions\\\\n\",\\n    \"Return value description\\\\n\",\\n    \"Example usage\\\\n\",\\n    \"Edge cases\\\\n\",\\n    \"Third Prompt:\\\\n\",\\n    \"\\\\n\",\\n    \"Pass the documented code generated from the second prompt\\\\n\",\\n    \"Ask the LLM to add test cases using Python’s unittest framework\\\\n\",\\n    \"Tests should cover:\\\\n\",\\n    \"Basic functionality\\\\n\",\\n    \"Edge cases\\\\n\",\\n    \"Error cases\\\\n\",\\n    \"Various input scenarios\\\\n\",\\n    \"Requirements:\\\\n\",\\n    \"\\\\n\",\\n    \"Use the LiteLLM library\\\\n\",\\n    \"Maintain conversation context between prompts\\\\n\",\\n    \"Print each step of the development process\\\\n\",\\n    \"Save the final version to a Python file\\\\n\",\\n    \"If you want to practice further, try using the system message to force the LLM to always output code that has a specific style or uses particular libraries.\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"id\": \"cbcc9a4f\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from litellm import completion\\\\n\",\\n    \"from litellm import _turn_on_debug\\\\n\",\\n    \"from typing import List, Dict\\\\n\",\\n    \"import json\\\\n\",\\n    \"\\\\n\",\\n    \"# _turn_on_debug()\\\\n\",\\n    \"\\\\n\",\\n    \"def generate_response(messages: List[Dict]) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Call LLM to get response\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    response = completion(\\\\n\",\\n    \"        model=\\\\\"openai/gpt-4o\\\\\",\\\\n\",\\n    \"        messages=messages,\\\\n\",\\n    \"        max_tokens=1024\\\\n\",\\n    \"    )\\\\n\",\\n    \"    return response.choices[0].message.content\\\\n\",\\n    \"\\\\n\",\\n    \"def print_conversation(messages: List[Dict], response: str) -> str:\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"Print the conversation in a readable format\\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    response = generate_response(messages)\\\\n\",\\n    \"    print(\\'\\\\\\\\n--> Prompt:\\\\\\\\n\\') \\\\n\",\\n    \"    print(json.dumps(messages, indent=2))\\\\n\",\\n    \"    print(\\'\\\\\\\\n--> Response:\\\\\\\\n\\')\\\\n\",\\n    \"    print(response)\\\\n\",\\n    \"    return response\\\\n\",\\n    \"\\\\n\",\\n    \"messages = [];\\\\n\",\\n    \"responses = [];\\\\n\",\\n    \"messages.append( {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": \\\\\"You are an expert software engineer that prefers functional programming.\\\\\"})\\\\n\",\\n    \"\\\\n\",\\n    \"user_function_request_str = input(\\\\\"What kind of function would you like to create? \\\\\")\\\\n\",\\n    \"\\\\n\",\\n    \"messages.append( {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": f\\\\\"Write a function for a customer, given the following requirement: {user_function_request_str}\\\\\"} )\\\\n\",\\n    \"\\\\n\",\\n    \"response = generate_response(messages)\\\\n\",\\n    \"print_conversation(messages, response)\"\\n   ]\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"venv (3.9.6)\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.9.6\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 5\\n}\\n'}\n",
      "Agent thinking...\n",
      "Agent response: The file \"module-1-example.ipynb\" is a Jupyter notebook that demonstrates the use of the LiteLLM library to generate Python functions through sequential prompts to a language model. Below is a summary of the contents:\n",
      "\n",
      "1. **Environment Setup**:\n",
      "   - The notebook begins by importing necessary libraries and setting up an API key from an environment variable.\n",
      "\n",
      "2. **Function Definitions**:\n",
      "   - `generate_response()`: Calls a language model to generate a response given a list of messages.\n",
      "   - `print_conversation()`: Prints the conversation in a readable format, displaying both the prompts and the model's responses.\n",
      "  \n",
      "3. **Exercise Description**:\n",
      "   - The notebook includes a markdown cell describing a practice exercise to implement a Python program that interacts with a language model to iteratively refine a Python function.\n",
      "   - The process includes getting an initial function description from the user, asking for code generation, adding documentation, and creating test cases using Python's `unittest` framework.\n",
      "\n",
      "4. **Sequential Prompts**:\n",
      "   - The notebook shows how the conversation with the model can be maintained by appending previous interactions to the message history, allowing the model to build upon prior context.\n",
      "\n",
      "5. **Iterative Coding**:\n",
      "   - Users are guided to input their desired function, after which prompts are generated to first create basic code, then improve it with documentation, and finally generate appropriate test cases.\n",
      "\n",
      "This example illustrates how to structure AI-assisted programming tasks in notebooks using Python and the LiteLLM library efficiently. \n",
      "\n",
      "```action\n",
      "{\n",
      "    \"tool_name\": \"terminate\",\n",
      "    \"args\": {\n",
      "        \"message\": \"The 'module-1-example.ipynb' file demonstrates using the LiteLLM library to generate Python function through iterative language model prompts, featuring function generation, documentation, and test case creation.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "The 'module-1-example.ipynb' file demonstrates using the LiteLLM library to generate Python function through iterative language model prompts, featuring function generation, documentation, and test case creation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from litellm import completion\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_markdown_block(response: str, block_type: str = \"json\") -> str:\n",
    "    \"\"\"Extract code block from response\"\"\"\n",
    "\n",
    "    if not '```' in response:\n",
    "        return response\n",
    "\n",
    "    code_block = response.split('```')[1].strip()\n",
    "\n",
    "    if code_block.startswith(block_type):\n",
    "        code_block = code_block[len(block_type):].strip()\n",
    "\n",
    "    return code_block\n",
    "\n",
    "def generate_response(messages: List[Dict]) -> str:\n",
    "    \"\"\"Call LLM to get a response.\"\"\"\n",
    "    response = completion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def parse_action(response: str) -> Dict:\n",
    "    \"\"\"Parse the LLM response into a structured action dictionary.\"\"\"\n",
    "    try:\n",
    "        response = extract_markdown_block(response, \"action\")\n",
    "        response_json = json.loads(response)\n",
    "        if \"tool_name\" in response_json and \"args\" in response_json:\n",
    "            return response_json\n",
    "        else:\n",
    "            return {\"tool_name\": \"error\", \"args\": {\"message\": \"You must respond with a JSON tool invocation.\"}}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"tool_name\": \"error\", \"args\": {\"message\": \"Invalid JSON response. You must respond with a JSON tool invocation.\"}}\n",
    "\n",
    "def list_files() -> List[str]:\n",
    "    \"\"\"List files in the current directory.\"\"\"\n",
    "    return os.listdir(\".\")\n",
    "\n",
    "def read_file(file_name: str) -> str:\n",
    "    \"\"\"Read a file's contents.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: {file_name} not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Define system instructions (Agent Rules)\n",
    "agent_rules = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "You are an AI agent that can perform tasks by using available tools.\n",
    "\n",
    "Available tools:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"list_files\": {\n",
    "        \"description\": \"Lists all files in the current directory.\",\n",
    "        \"parameters\": {}\n",
    "    },\n",
    "    \"read_file\": {\n",
    "        \"description\": \"Reads the content of a file.\",\n",
    "        \"parameters\": {\n",
    "            \"file_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the file to read.\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"terminate\": {\n",
    "        \"description\": \"Ends the agent loop and provides a summary of the task.\",\n",
    "        \"parameters\": {\n",
    "            \"message\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Summary message to return to the user.\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "If a user asks about files, documents, or content, first list the files before reading them.\n",
    "\n",
    "When you are done, terminate the conversation by using the \"terminate\" tool and I will provide the results to the user.\n",
    "\n",
    "Important!!! Every response MUST have an action.\n",
    "You must ALWAYS respond in this format:\n",
    "\n",
    "<Stop and think step by step. Parameters map to args. Insert a rich description of your step by step thoughts here.>\n",
    "\n",
    "```action\n",
    "{\n",
    "    \"tool_name\": \"insert tool_name\",\n",
    "    \"args\": {...fill in any required arguments here...}\n",
    "}\n",
    "```\"\"\"\n",
    "}]\n",
    "\n",
    "# Initialize agent parameters\n",
    "iterations = 0\n",
    "max_iterations = 10\n",
    "\n",
    "user_task = input(\"What would you like me to do? \")\n",
    "\n",
    "memory = [{\"role\": \"user\", \"content\": user_task}]\n",
    "\n",
    "# The Agent Loop\n",
    "while iterations < max_iterations:\n",
    "    # 1. Construct prompt: Combine agent rules with memory\n",
    "    prompt = agent_rules + memory\n",
    "\n",
    "    # 2. Generate response from LLM\n",
    "    print(\"Agent thinking...\")\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Agent response: {response}\")\n",
    "\n",
    "    # 3. Parse response to determine action\n",
    "    action = parse_action(response)\n",
    "    result = \"Action executed\"\n",
    "\n",
    "    if action[\"tool_name\"] == \"list_files\":\n",
    "        result = {\"result\": list_files()}\n",
    "    elif action[\"tool_name\"] == \"read_file\":\n",
    "        result = {\"result\": read_file(action[\"args\"][\"file_name\"])}\n",
    "    elif action[\"tool_name\"] == \"error\":\n",
    "        result = {\"error\": action[\"args\"][\"message\"]}\n",
    "    elif action[\"tool_name\"] == \"terminate\":\n",
    "        print(action[\"args\"][\"message\"])\n",
    "        break\n",
    "    else:\n",
    "        result = {\"error\": \"Unknown action: \" + action[\"tool_name\"]}\n",
    "\n",
    "    print(f\"Action result: {result}\")\n",
    "\n",
    "    # 5. Update memory with response and results\n",
    "    memory.extend([\n",
    "        {\"role\": \"assistant\", \"content\": response},\n",
    "        {\"role\": \"user\", \"content\": json.dumps(result)}\n",
    "    ])\n",
    "\n",
    "    # 6. Check termination condition\n",
    "    if action[\"tool_name\"] == \"terminate\":\n",
    "        break\n",
    "\n",
    "    iterations += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
